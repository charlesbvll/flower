name: Baseline request 2
description: Suggest a new baseline
labels: ["new baseline, good first issue"]
title: Add [new_baseline_name] baseline

body:
  - type: input
    attributes:
      label: Paper
      description: What paper would you like implemented as a baseline ?
      placeholder: "Authors, year, title"
    validations:
      required: true
  - type: input
    attributes:
      label: Link
      description: Provide a link (ideally an arxiv.org/abs/* link) to the abstract of the paper.
      placeholder: https://arxiv.org/abs/1608.03452
    validations:
      required: true
  - type: textarea
    attributes:
      label: Maybe give motivations about why the paper should be implemented as a baseline.
    validations:
      required: false
  - type: textarea
    attributes:
      label: Is there something else you want to add?
  - type: markdown
    attributes:
      value: |
        #### If you want to propose a new baseline, please check the PRs if someone already works on it.

        ## Implementation

        ### Prep - understand the scope

        It’s recommended to do the following items in that order:

        - [ ]  Read the paper linked above
        - [ ]  Create the directory structure in Flower Baselines (just the `__init__.py`files and a README)
        - [ ]  Before starting to write code, 
        write down all of the specs of this experiment in a README (dataset, partitioning, model, number of clients, all hyperparameters, …)
        - [ ]  Open a draft PR

        ### Implement - make it work

        Everything up to this point should be pretty mechanical, 
        the goal is to get a simple version of this working as quickly as possible, 
        which means a model that’s starting to converge (doesn’t have to be good).

        - [ ]  Implement some form of dataset loading and partitioning in a separate `dataset.py` (doesn’t have to match the paper exactly)
        - [ ]  Implement the model in PyTorch
        - [ ]  Write a test that shows that the model has the number of parameters mentioned in the paper
        - [ ]  Implement the federated learning setup outlined in the paper, maybe starting with fewer clients
        - [ ]  Plot accuracy and loss
        - [ ]  Run it and check if the model starts to converge

        ### Align - make it correct

        - [ ]  Implement the exact data partitioning outlined in the paper
        - [ ]  Use the exact hyperparameters outlined in the paper

        ### Tuning - make it converge

        - [ ]  Make it converge to roughly the same accuracy that the paper states
        - [ ]  Mark the PR as ready
